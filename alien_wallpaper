#!/usr/bin/env python3
"""
Download images from Reddit.

```bash
$ python3 alien_wallpaper.py --help
```
"""

import argparse
import codecs
from functools import partial
import json
import logging
import multiprocessing
import pathlib
import re
import shutil
import urllib.parse
import urllib.request

IMAGES_PER_SUBREDDIT = 25
DEFAULT_SUBREDDITS = ['ArchitecturePorn', 'CityPorn', 'EarthPorn', 'SkyPorn',
                      'spaceporn', 'winterporn', 'quoteporn']

FILE_EXTENSION_PROG = re.compile('.*.(jpg|jpeg|gif|png)$', re.IGNORECASE)
USER_AGENT = 'AlienWallpaper by /u/WolfBlackout'


def valid_post(post):
    return not post['data']['is_self'] and bool(FILE_EXTENSION_PROG.match(post['data']['url']))


def filename(post):
    reddit_id = post['data']['id']
    ext = FILE_EXTENSION_PROG.match(post['data']['url']).group(1)
    return reddit_id + '.' + ext


def download_image(post, out_dir):
    out_dir = pathlib.Path(out_dir)
    if not out_dir.is_absolute():
        out_dir = pathlib.Path(pathlib.Path.cwd(), out_dir)
    path = out_dir / filename(post)
    logging.info(path)

    url = post['data']['url']
    try:
        with urllib.request.urlopen(url) as response, path.open('wb+') as out_file:
            shutil.copyfileobj(response, out_file)
    except urllib.error.HTTPError:
        logging.exception('download_image')


def fetch_metadata(url, count=25, after=''):
    params = urllib.parse.urlencode({'count': count, 'after': after})
    url = url + '.json?' + params
    req = urllib.request.Request(url=url, headers={'User-Agent': USER_AGENT})
    reader = codecs.getreader('utf-8')
    return json.load(reader(urllib.request.urlopen(req)))


def download_images(subreddit, n, out_dir):
    data = fetch_metadata(subreddit)
    nimages = 0
    while True:
        for post in data['data']['children']:
            if valid_post(post):
                download_image(post, out_dir)
                nimages += 1
                if nimages >= n:
                    return
        data = fetch_metadata(subreddit, after=data['data']['after'])


def subreddit_to_url(subreddit):
    return 'https://www.reddit.com/r/{}'.format(subreddit)


def download_all_images(subreddits, n, out_dir, par=True):
    """
    Download n images for each subreddit to out_dir.

    :param par: use multiprocessing.Pool to download images in parallel

    """
    if par:
        pool = multiprocessing.Pool()
        download_images_to_dir = partial(download_images, n=n, out_dir=out_dir)
        pool.map(download_images_to_dir, subreddits)
        pool.close()
        pool.join()
    else:
        for subreddit in subreddits:
            download_images(subreddit, n, out_dir)


def parse_cli():
    parser = argparse.ArgumentParser(description='Download Reddit images.')
    parser.add_argument('--subreddits', nargs='*')
    parser.add_argument('--multireddit', nargs='?')
    parser.add_argument('--out', required=True)
    return parser.parse_args()


def main():
    # Parse CLI arguments to get subreddits and output directory
    args = parse_cli()

    # Make list of subreddit urls
    if not args.subreddits and not args.multireddit:
        subreddits = [subreddit_to_url(s) for s in DEFAULT_SUBREDDITS]
    else:
        subreddits = []
        if args.subreddits:
            subreddits.extend([subreddit_to_url(s) for s in args.subreddits])
        if args.multireddit:
            user, multi = args.multireddit.split('/')
            url = 'https://www.reddit.com/user/{}/m/{}'.format(user, multi)
            subreddits.append(url)

    download_all_images(subreddits, IMAGES_PER_SUBREDDIT, args.out)

if __name__ == '__main__':
    main()
